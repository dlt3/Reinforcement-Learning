{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZWzLaaST+Af8kuAIs4DBu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["  ## 강화학습\n","\n","  심층 학습 알고리즘은 다양한 종류의 문제와 과제에 적용할 수 있다. 심층학습 이전의 자동 이미지 처리 기법들은 복잡한 이미지를 제대로 처리하지 못했기 때문에 용도가 아주 제한적이였다.\n","\n","  이미지 분류 알고리즘과는 달리 강화학습 알고리즘은 데이터와 동적으로 상호작용한다. 강화학습 알고리즘은 끊임없이 데이터를 소비하면서 다음데 취할 동작을 결정한다. 그 동작(행위)에 따라 이후에 입력되는 데이터가 달라질 수 있다.\n","\n","  강화학습(reinforcement learning)은 제어 과제의 표현 및 해결을 위한 일반적인 틀(프레임웨크)이다. 강화학습의 틀을 벗어나지 않는 한에서, 주어진 구체적인 과제에 맞는 특정한 알고리즘을 임의로 선택할 수 있다.\n","   강화핛브을 위한 알고리즘으로는 심층 학습 알고리즘이 흔히 쓰이며 이는 심층학습에 복잡한 데이터를 효율적으로 처리하는 능력이 있기 때문이다.\n","  심층학습 알고리즘을 이용한 강화학습을 심층 강화학습이라고 부른다."],"metadata":{"id":"6NLTqHQbsjRq"}},{"cell_type":"markdown","source":["통상적인 이미지 분류와 비슷한 과제들은 대부분 지도학습의 범주에 속한다. 지도학ㄷ습에서는 학습 알고리즘(학습 모형)을 훈련할 때 '정답'을 제시해서 알고리즘을 올바른 방향으로 이끈다. 학습 알고리즘은 청므에는 무작위로 분류 또는 예측 결과를 산출하되, 훈련 데이터에 포함된 정답과 자신의 결과를 비교해서 점차 매개변수들을 조정해 나간다. 따라서 지도학습을 위해서는 정답이 포함된 훈련 데이터, 소위 분류된 데이터(labeled data)를 사람이 만들어야 하며 이는 상황에 따라서 만들기 번거롭거나 어려운 수 있다.\n"," 반면 강화학습에서는 각 단계의 '정답'을 제공하지 않는다. 최종적인 목표(goal)와 피해야 할 상황을 지정하고, 행동의 결과에 따라 적절한 보상을 제공하기만 하면 된다. 강화학습에서는 알고리즘이 어떤 상위 목표(고수준 목표)를 달성하면 보상을 제공한다. 또한, 바람직하지 않은 행동을 했을때는 벌점을 줄 수도 있다.\n","\n"," 예를 들어 자율주행 과제라면 \"충돌 사고 없이 A 지점에서 B지점으로 간다\"를 상위 목표로 삼으면 될 것이다. 만일 차량이 B 지점에 좀더 가까워지는 동작을 취했다면 보상을 제공하고, 뭔가와 충돌하면 별점을 제공한다. 이런 훈련 과정을 실제 도로에서 실행한다면 피해가 클 것이므로, 먼저 시뮬레이션을 통한 충분한 훈련이 반복되어야 할 것이다."],"metadata":{"id":"ZmbZ83RP8ghO"}},{"cell_type":"markdown","source":["강화학습 알고리즘의 주된 목적은 보상을 최대화하는 것이다. 보상을 최대화하려면 알고리즘은 상위 목표를 달성하기 위한 기본 능력들을 배워야 한다. 알고리즘이 적절히 행동을 취했을 때 양의 보상을 제공하는 것과 더불어, 먼가 부적절한 동작을 취했을 때 음의 보상을 제공할 수도 있다. 그러면 알고리즘은 좋은 동작들을 배울 뿐만 아니라 피해야 할 동작들도 배우게 된다. 이처럼 보상과 벌점을 통해서 모형의 행동 방식을 긍정적으로 또는 부정적으로 강화한다는 점에서 강화학습이라는 이름이 붙었다."],"metadata":{"id":"_CvoM6vGChld"}},{"cell_type":"markdown","source":["#### 그림 1.7 추가"],"metadata":{"id":"3YTMUs6oDIMl"}},{"cell_type":"markdown","source":["### 동적 계획법과 몬테카를로 방법"],"metadata":{"id":"75TcAZQ2DWGD"}},{"cell_type":"markdown","source":["동적 계획법(Dynamic programming, DP) : 복잡한 고 수준의 문제를 점점 더 작은 부분 문제들로 분해하고, 추가 정보 없이 풀수 있을 정도로 작은 부분 문제들에 도달하면 그것들을 풀어서 전체적인 해답을 조립하는 기법\n","\n","일반적인 경우 문제 해결 스펙트럼의 경우 주변 환경 및 상황에 대한 완벽한 파악이 선행되어 완벽한 모형(model)이 갖추어져야 하지만 그렇지 않은 경우는 시행착오가 필요하다. 이러한 시행착오 전략은 크게 몬테가를로 방법(Monte Carlo method)의 범주에 속하며 이는 본질적으로 환경의 무작위 표집(random sampling)에 해당한다."],"metadata":{"id":"fgVKZaZiDbQc"}},{"cell_type":"markdown","source":["### 강화학습의 틀"],"metadata":{"id":"tyO6cggfYh5F"}},{"cell_type":"markdown","source":[" 강화학습의 틀은 모든 강화학습 문제의 서술에 사용할 수 있는 핵심 용어와 개념의 총합에 해당한다. 이 틀은 다른 기술자나 비슷한 문제 부냏 기법을 적용하기 좋게 문제를 공식화하도록 강제하는 역할도 한다.\n","\n","강화학습의 틀을 적용하려면 우선 전체적인 목표 또는 목적(objective)를 정의해야 한다. 이를 위해서는 목적함수(objective function)을 만드는 것이 매우 중요하며 목적함수가 보여주는 수치 자체가 중요한 것이 아니라 그 수치를 가능한 한 최소화하는 것이 중요하다고 할 수 있다. 강화학습 알고리즘은 입력 데이터에 대한 이 목적함수(오차함수)의 값을 최소화하는 수단이다.\n","\n","입력 데이터는 환경(environment)이 생성한다. 일반적으로 강화학습 과제(또는 제어 과제)의 환경에는 목적 달성과 관련된 데이터를 산출하는 모든 동적 과정이 포함된다. 우리는 항상 어떤 환경 안에서 살아가며, 일상의 목적들을 달성하기 위해 우리는 환경을 눈과 귀로 끊임없이 관찰한다. 환경은 하나의 동적 과정(dynamic process), 즉 시간의 함수이르모, 크기와 형식이 다양한 데이터를 끊임없이 산출한다. 알고리즘 구현의 편의를 위해서는 연속적인 환경 데이터 스트림을 일련의 이산적인(discrete) 조각들로 나누고 묶을 필요가 있다. 그런 개별 데이터 조각을 (환경의) 상태(state)라고 부른다. 강화학습 알고리즘은 이산적인 시간 단계들에서 이산적인 상태 데이터를 입력받는다. 상태는 특정 시점에서 환경에 관한 우리의 지식을 반영하는데, 이는 디지털 카메라로 풍경이나 피사체의 순간적인 모습을 갈무리한 것과 비슷하다.\n","\n"," 강화학습과 통상적인 지도학습의 중요한 차이점 하나는 강화학습에서는 예측이나 분류에서 그치지 않고 어떤 행동 또는 동작을 결정하고 실행한다는 것이다. 알고리즘이 취한 동작은 호나경에 영향을 주어서 미래의 입력이 달라지게 만든다. \"동작을 취한다\"는 것은 가오하학습의 핵심 개념 중 하나이다. 동작을 취한다는 것 자체의 의미는 일상에서 말하는 것과 그리 다르지 않다. 중요한 것은, 강화학습 알고리즘이 취하는 모든 동작은 환경의 현재 상태를 분석하고 그 분석 정보에 기초해서 최선의 결정을 내리고자 하는 시도의 결과라는 점이다.\n","\n","강화학습의 틀의 나머지 조각은 동작을 취한 후 학습 ㅇ라고리즘에 제공되는 보상(reward)이다. 이 보상은 전젳적인 목표를 항해 학습 알고리즘이 얼마나 잘 안아가고 있는지를 말해주는(국소적인) 신호에 해당한다. 이때 보상은 긍정적인 신호(\"잘 하고 있어, 계속 그렇게 하면 돼\")일 수도 있고 부정적인 신호(\"그러면 안 돼\")일 수도 있지만, 둘다 그냥 '보상'이라고 부른다.\n"," 환경의 다음 상태에 대해 더 나은 동작을 취하도록 알고리즘이 자신을 갱신하는 데 활용할 수 있는 신호는 이 보상이 유일한다. 지도학습과는 달리 '정답'은 주어지지 않는다. 강화학습 알고리즘에 에이전트(agent, 대행자)라는 좀 더 그럴듯한 이름을 가지고 있으며 동작을 취하거나 결정을 내리는 모든 학습 알고리즘을 에이전트라고 부른다."],"metadata":{"id":"pxWWGPcdNzLT"}},{"cell_type":"markdown","source":["그림 1.8 추가"],"metadata":{"id":"LbkXnnfSgq8D"}},{"cell_type":"markdown","source":[" 우리가 환경에 관한 완전한 지식을 제공하지 않는 한, 에이전트는 으너 정도 시행착오를 거치게 된다. 운이 좋아서 에이전트의 학습이 아주 잘 되었다면, 에이전트는 원래 훈련한 환경과는 다른 환경에서도 냉방 비용을 줄일 수 있을 것이다. 에이전트는 하나의 학습자 또는 학습 모형이므로, 에이전트를 구현하려면 어떤 종류이든 학습 알고리즘이 필요하다. 그러나 강화학습은 특정 학습 알고리즘에 관한 것이 아니라 문제와 그 해법의 유형에 관한 것을 의미하며 심층 신경망 이외의 학습 알고리즘도 강화학습 에이전트에 사용할 수 있다.\n","\n"," 에이전트의 유일한 목적은 장기적인 기대 보상을 최대화하는 것이다. 에이전트는 상태 정보를 처리하고, 적절한 동작을 결정해서 취하고, 그에 따른 보상을 받고 다시 새 상태 정보를 처리하고, 동작을 취하고, 등등의 과정을 반복한다. 이 모든 과정이 잘 진행된다면 에이전트는 환경을 이해하게 되고, 매 단계에서 좋은 결정을 내리게 된다. 이러한 일반적인 메커니즘이 자율주행이나 챗봇, 로봇, 전산매매(자동 주식 거래), 보건 등 다양한 분야에 적용된다.\n","\n","  이 책의 대부분은 주어진 난제를 강화학습의 틀에 맞게 조직화하고 적절히 강력한 학습 알고리즘(에이전트)을 구현해서 해결하는 방법을 다룬다. 이러한 경우 에이전트를 훈련할 환경을 매번 직접 만들어야 한다는 부담이 상당히 클 것이다."],"metadata":{"id":"YtTU6A-Mgt2-"}},{"cell_type":"markdown","source":["#### 1.5 강화학습으로 할 수 있는 일\n","\n"," 지도학습의 최근 성과가 중요하고 유용하긴 하지만, 지도학습 접근 방식으로는 인공 일반 지능(artificial general intelligence, AGE; 범용 인공지능)에 도달하기 어렵다. 우리의 궁극적인 목표는 사람의 지도나 개입이 (거의)없어도 다양한 문제들을 해결하는, 그리고 자신의 지식화 능력을 다른 문제 영역에도 전달할 수 있는 범용 학습 기계를 만드는 것이다. 풍부한 데이터를 확보한 대기업들은 지도학습 접근 방식에서 이득을 얻을 수 있지만, 소규모 기업과 조직은 데이터 부족 때문에 기계학습의 위력을 충분히 활용하기 어렵다. 범용 학습 알고리즘은 그런 불평등한 상황을 해소해 줄 것이며, 그런 범용 학습 알고리즘을 만드는 접근 방식으로 현재 가장 유망한 것이 바로 강화학습이다.\n","\n"],"metadata":{"id":"GkiPQ3IP1hOm"}},{"cell_type":"markdown","source":["#### 끈 그림\n","\n","그림 1.14\n","\n","위의 그림은 끈 그림의 한 종류로, 층이 두 개인 신경망을 고수준에서 나타낸 것이다. 기계학습에 행렬 연산과 벡터 연산이 많이 관여하는데, 끈 그림은 그런 종류의 연산에 특히 잘 맞는다. 끝 그림은 또한 복잡한 과정(프로세스)을 서술하는 데도 탁월하다. 하나의 프로세스를 여러 추상 수준에서 묘사할 수 있기 때문이다."],"metadata":{"id":"1H7EOaV35ukR"}},{"cell_type":"markdown","source":["#### 요약\n","\n","- 강화학습은 기계학습의 한 종류이다. 환경 안에서 보상을 최대화함으로써 학습 대상을 배우는 강화학습 알고리즘은 결정을 내리거나 동작을 취해야 하는 문제에 유용하다. 원칙적으로 가오하학습에는 그 어떤 통계적 학습 모형도 적용할 수 있지만, 최근 들어 가장 효과적이고 이니있는 모형은 심층 신경망이다.\n","\n","- 모든 강화학습 문제의 핵심은 에이전트이다. 에이전트는 입력을 처리해서 다음에 취할 행동을 결정한다.\n","\n","- 환경은 에이전트가 그 안에서 행동하는, 잠재적으로 동적인 조건들의 집합이다. 즉 에이전트를 위해 입력 데이터를 생성하는 모든 과정을 환경이라고 간주할 수 있다.\n","\n","- 상태는 특정 순간에서의 환경의 스냅숏으로, 에이전트는 주어진 상태에 기초해서 결정을 내린다. 환경은 끊임없이 변하는 조건들의 집합일 때가 많지만, 효과적인 처리를 위해 이산적(분연속적)인 순간들에서 환경에서 표본을 추출하고 그 표본을 환경의 한 상태로서 에이전트에게 입력한다.\n","\n","- 에이전트는 환경에 대해 수행할 동작을 결정하며, 동작은 다시 에이전트의 환경을 바꾼다.\n","\n","- 에이전트가 종작을 취하면 환경은 에이전트에게 긍정적 또는 부정적 강화 신호로서의 보상을 제공한다. 보상은 에이전트가 받는 유일한 학습 신호이다. 모든 강화학습 알고리즘(에이전트)의 목적은 보상을 최대화하는 것이다.\n","\n","- 모든 강화학습 알고리즘은 에이전트가 입력 데이터(환경의 상태)를 받고 그것을 평가하고, 그에 기초해서 현재 상태에서 가능한 동작 중 하나를 선택하고, 그 동작을 수행해서 환경을 바꾸고, 환경이 보상 신호화 다음 입력 데이터를 에이전트에게 제공하는 루프가 계속 반복되는 구조로 일반화된다. 에이전트를 심층 신경망으로 구현한 경우 이 루프는 보상 신호에 기초해서 손실함수를 평가하고 그것을 신경망을 따라 역전파해서 신경망의 매개변수들을 갱신하는(보상이 더 커지도록 ) 형태이다. 이 과정을 반복하므로써 에이전트의 성능이 점점 개선된다.\n"],"metadata":{"id":"YdpwziWB6l2k"}},{"cell_type":"code","source":[],"metadata":{"id":"RnugaW9lnVG9"},"execution_count":null,"outputs":[]}]}